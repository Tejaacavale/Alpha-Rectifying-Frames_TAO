{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import tqdm\n",
    "import os\n",
    "import mcbe\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "X_3 = X[:,[0,2,3]]\n",
    "\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_3)\n",
    "#m = np.amax(X_scaled)\n",
    "#X_ball = X_scaled/m\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homo(A,filename = 'Test',name='w1',epoch=1):\n",
    "    np.set_printoptions(precision=4,threshold=10_000,suppress = True)\n",
    "    if torch.is_tensor(A):\n",
    "        A = A.detach().numpy()\n",
    "    h = np.ones((A.shape[0],1))\n",
    "    A_homo = np.concatenate((h,A), axis = 1)\n",
    "    mat = np.matrix(A_homo)\n",
    "    with open(filename+'/'+name+'_ep'+str(epoch)+'.txt','wb') as f:\n",
    "        for line in mat:\n",
    "            np.savetxt(f, line, fmt='%.2f')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the first layer\n",
    "l1 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, l1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, l1)\n",
    "        #self.layer2 = nn.Linear(20, 10)\n",
    "        self.layer3 = nn.Linear(l1, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        #x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Linear(in_features=3, out_features=200, bias=True)\n",
       "  (layer3): Linear(in_features=200, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model     = Model(X_train.shape[1],l1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 659.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80000001, 0.76666665, 0.76666665, 0.83333331, 0.83333331,\n",
       "       0.83333331, 0.83333331, 0.83333331, 0.93333334, 0.93333334,\n",
       "       0.89999998, 0.93333334, 0.89999998, 0.89999998, 0.93333334,\n",
       "       0.96666664, 0.96666664, 0.96666664, 0.96666664, 0.96666664,\n",
       "       0.96666664, 0.96666664, 0.96666664, 0.96666664, 0.96666664])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize lists\n",
    "\n",
    "EPOCHS  = 25\n",
    "\n",
    "weights = []\n",
    "weights_norm = []\n",
    "norms = []\n",
    "biases = np.zeros([EPOCHS + 1,l1])\n",
    "\n",
    "w1 = model.layer1.weight\n",
    "b1 = model.layer1.bias\n",
    "m1 = w1.shape[0]\n",
    "n1 = w1.shape[1]\n",
    "norm = w1.pow(2).sum(keepdim=True,dim=1).sqrt()\n",
    "w1_norm = torch.div(w1,norm)\n",
    "w1_norm[w1_norm == np.inf] = 0\n",
    "\n",
    "    \n",
    "weights.append(w1.detach().numpy())\n",
    "weights_norm.append(w1_norm.detach().numpy())\n",
    "norms.append(norm.detach().numpy())\n",
    "biases[0,:] = b1.detach().numpy()\n",
    "\n",
    "#epoch counter\n",
    "k = 1\n",
    "\n",
    "X_train = Variable(torch.from_numpy(X_train)).float()\n",
    "y_train = Variable(torch.from_numpy(y_train)).long()\n",
    "X_test  = Variable(torch.from_numpy(X_test)).float()\n",
    "y_test  = Variable(torch.from_numpy(y_test)).long()\n",
    "\n",
    "loss_list     = np.zeros((EPOCHS,))\n",
    "accuracy_list = np.zeros((EPOCHS,))\n",
    "\n",
    "for epoch in tqdm.trange(EPOCHS):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss_list[epoch] = loss.item()\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
    "        accuracy_list[epoch] = correct.mean()\n",
    "    \n",
    "    \n",
    "    \n",
    "    w1 = model.layer1.weight\n",
    "    b1 = model.layer1.bias\n",
    "    norm = w1.pow(2).sum(keepdim=True,dim=1).sqrt()\n",
    "    w1_norm = torch.div(w1,norm)\n",
    "    w1_norm[w1_norm == np.inf] = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    weights.append(w1.detach().numpy())\n",
    "    weights_norm.append(w1_norm.detach().numpy())\n",
    "    norms.append(norm.detach().numpy())\n",
    "    biases[k,:] = b1.detach().numpy()\n",
    "    \n",
    "    k = k+1\n",
    "    \n",
    "\n",
    "\n",
    "accuracy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_evaluation_iter = 100\n",
    "times_to_blowup = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate blowup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [01:29<00:21,  1.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# rowbind the original data and the noisy data by adding the noisy data to the original data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m X_train_blowup \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((X_train,X_train_noisy),\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m est_alpha_blowup, subframes_blowup, est_points_blowup \u001b[38;5;241m=\u001b[39m mcbe\u001b[38;5;241m.\u001b[39mbe_given_points(polytope\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(weights)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],points\u001b[38;5;241m=\u001b[39mX_train_blowup, give_subframes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m percent_inj_blowup \u001b[38;5;241m=\u001b[39m mcbe\u001b[38;5;241m.\u001b[39mcheck_injectivity_naive(W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(weights)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], b\u001b[38;5;241m=\u001b[39mest_alpha_blowup, points\u001b[38;5;241m=\u001b[39mX_test,\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     16\u001b[0m percent_inj_blowup_list\u001b[38;5;241m.\u001b[39mappend(percent_inj_blowup)\n",
      "File \u001b[1;32mc:\\Users\\heckert\\Documents\\GitHub\\Alpha-rectifying-frames\\mcbe\\mcbe.py:298\u001b[0m, in \u001b[0;36mbe_given_points\u001b[1;34m(polytope, points, init, give_subframes, sample_on_sphere)\u001b[0m\n\u001b[0;32m    293\u001b[0m         alpha[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin([alpha[idx], corr_x_vert[idx]])\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m points:\n\u001b[1;32m--> 298\u001b[0m     corr_x_vert \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mdot(point, phi) \u001b[38;5;28;01mfor\u001b[39;00m phi \u001b[38;5;129;01min\u001b[39;00m polytope]\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;66;03m#find subframes\u001b[39;00m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m give_subframes \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\heckert\\Documents\\GitHub\\Alpha-rectifying-frames\\mcbe\\mcbe.py:298\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    293\u001b[0m         alpha[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin([alpha[idx], corr_x_vert[idx]])\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m points:\n\u001b[1;32m--> 298\u001b[0m     corr_x_vert \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mdot(point, phi) \u001b[38;5;28;01mfor\u001b[39;00m phi \u001b[38;5;129;01min\u001b[39;00m polytope]\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;66;03m#find subframes\u001b[39;00m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m give_subframes \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "percent_inj_blowup_list = []\n",
    "\n",
    "for iter in tqdm.tqdm(range(num_evaluation_iter)):\n",
    "    \n",
    "    X_train_noisy = X_train + np.random.normal(0,0.5,X_train.shape)\n",
    "    for i in range(times_to_blowup-1):\n",
    "        X_train_noisy = torch.cat((X_train_noisy,X_train + np.random.normal(0,0.5,X_train.shape)),0)\n",
    "        \n",
    "    # rowbind the original data and the noisy data by adding the noisy data to the original data\n",
    "    X_train_blowup = torch.cat((X_train,X_train_noisy),0)\n",
    "\n",
    "    est_alpha_blowup, subframes_blowup, est_points_blowup = mcbe.be_given_points(polytope=np.array(weights)[-1],points=X_train_blowup, give_subframes=True)\n",
    "\n",
    "    percent_inj_blowup = mcbe.check_injectivity_naive(W = np.array(weights)[-1], b=est_alpha_blowup, points=X_test,iter=X_test.shape[0])\n",
    "\n",
    "    percent_inj_blowup_list.append(percent_inj_blowup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate random added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:55<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "percent_inj_rd_added = []\n",
    "\n",
    "for iter in tqdm.tqdm(range(num_evaluation_iter)):\n",
    "\n",
    "    X_train_rd = np.random.normal(0,1,X_train.shape)\n",
    "    X_train_rd = torch.from_numpy(X_train_rd).float()\n",
    "    for i in range(times_to_blowup-1):\n",
    "        X_train_rd = torch.cat((X_train_rd, torch.from_numpy(np.random.normal(0,1,X_train.shape))),0)\n",
    "        \n",
    "    # rowbind the original data and the noisy data by adding the noisy data to the original data\n",
    "    X_train_add_rd = torch.cat((X_train,X_train_rd),0)\n",
    "\n",
    "    est_alpha_rd, subframes_rd, est_points_rd = mcbe.be_given_points(polytope=np.array(weights)[-1],points=X_train_add_rd, give_subframes=True)\n",
    "\n",
    "    percent_inj_rd = mcbe.check_injectivity_naive(W = np.array(weights)[-1], b=est_alpha_rd, points=X_test,iter=X_test.shape[0])\n",
    "\n",
    "    percent_inj_rd_added.append(percent_inj_rd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate standard mcbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.22it/s]\n"
     ]
    }
   ],
   "source": [
    "percent_inj_mcbe_list = []\n",
    "for iter in tqdm.tqdm(range(num_evaluation_iter)):\n",
    "\n",
    "    est_alpha, subframes, est_points =mcbe.mcbe(polytope=np.array(weights)[-1],N=X_train_blowup.shape[0],distribution=\"ball\",radius=np.max(np.array(X_train)), give_subframes=True,sample_on_sphere=False)\n",
    "\n",
    "    percent_inj_mcbe = mcbe.check_injectivity_naive(W = np.array(weights)[-1], b=est_alpha, points=X_test,iter=X_test.shape[0])\n",
    "\n",
    "    percent_inj_mcbe_list.append(percent_inj_mcbe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(percent_inj_blowup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(percent_inj_rd_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7996666666666666"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(percent_inj_mcbe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
