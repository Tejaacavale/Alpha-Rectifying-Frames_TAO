{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import tqdm\n",
    "import os\n",
    "import mcbe\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "X_3 = X[:,[0,2,3]]\n",
    "\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_3)\n",
    "#m = np.amax(X_scaled)\n",
    "#X_ball = X_scaled/m\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homo(A,filename = 'Test',name='w1',epoch=1):\n",
    "    np.set_printoptions(precision=4,threshold=10_000,suppress = True)\n",
    "    if torch.is_tensor(A):\n",
    "        A = A.detach().numpy()\n",
    "    h = np.ones((A.shape[0],1))\n",
    "    A_homo = np.concatenate((h,A), axis = 1)\n",
    "    mat = np.matrix(A_homo)\n",
    "    with open(filename+'/'+name+'_ep'+str(epoch)+'.txt','wb') as f:\n",
    "        for line in mat:\n",
    "            np.savetxt(f, line, fmt='%.2f')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the first layer\n",
    "l1 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, l1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, l1)\n",
    "        #self.layer2 = nn.Linear(20, 10)\n",
    "        self.layer3 = nn.Linear(l1, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        #x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Linear(in_features=3, out_features=200, bias=True)\n",
       "  (layer3): Linear(in_features=200, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model     = Model(X_train.shape[1],l1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 557.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.73333335, 0.73333335, 0.73333335, 0.73333335, 0.80000001,\n",
       "       0.83333331, 0.83333331, 0.86666667, 0.89999998, 0.93333334,\n",
       "       0.96666664, 0.96666664, 0.96666664, 0.96666664, 0.96666664,\n",
       "       0.96666664, 0.96666664, 0.96666664, 0.96666664, 0.96666664,\n",
       "       0.96666664, 0.96666664, 0.96666664, 0.96666664, 0.96666664])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize lists\n",
    "\n",
    "EPOCHS  = 25\n",
    "\n",
    "weights = []\n",
    "weights_norm = []\n",
    "norms = []\n",
    "biases = np.zeros([EPOCHS + 1,l1])\n",
    "\n",
    "w1 = model.layer1.weight\n",
    "b1 = model.layer1.bias\n",
    "m1 = w1.shape[0]\n",
    "n1 = w1.shape[1]\n",
    "norm = w1.pow(2).sum(keepdim=True,dim=1).sqrt()\n",
    "w1_norm = torch.div(w1,norm)\n",
    "w1_norm[w1_norm == np.inf] = 0\n",
    "\n",
    "    \n",
    "weights.append(w1.detach().numpy())\n",
    "weights_norm.append(w1_norm.detach().numpy())\n",
    "norms.append(norm.detach().numpy())\n",
    "biases[0,:] = b1.detach().numpy()\n",
    "\n",
    "#epoch counter\n",
    "k = 1\n",
    "\n",
    "X_train = Variable(torch.from_numpy(X_train)).float()\n",
    "y_train = Variable(torch.from_numpy(y_train)).long()\n",
    "X_test  = Variable(torch.from_numpy(X_test)).float()\n",
    "y_test  = Variable(torch.from_numpy(y_test)).long()\n",
    "\n",
    "loss_list     = np.zeros((EPOCHS,))\n",
    "accuracy_list = np.zeros((EPOCHS,))\n",
    "\n",
    "for epoch in tqdm.trange(EPOCHS):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss_list[epoch] = loss.item()\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
    "        accuracy_list[epoch] = correct.mean()\n",
    "    \n",
    "    \n",
    "    \n",
    "    w1 = model.layer1.weight\n",
    "    b1 = model.layer1.bias\n",
    "    norm = w1.pow(2).sum(keepdim=True,dim=1).sqrt()\n",
    "    w1_norm = torch.div(w1,norm)\n",
    "    w1_norm[w1_norm == np.inf] = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    weights.append(w1.detach().numpy())\n",
    "    weights_norm.append(w1_norm.detach().numpy())\n",
    "    norms.append(norm.detach().numpy())\n",
    "    biases[k,:] = b1.detach().numpy()\n",
    "    \n",
    "    k = k+1\n",
    "    \n",
    "\n",
    "\n",
    "accuracy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_evaluation_iter = 100\n",
    "times_to_blowup = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate blowup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mcbe' has no attribute 'be_given_points'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# rowbind the original data and the noisy data by adding the noisy data to the original data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m X_train_blowup \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((X_train,X_train_noisy),\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m est_alpha_blowup, subframes_blowup, est_points_blowup \u001b[38;5;241m=\u001b[39m mcbe\u001b[38;5;241m.\u001b[39mbe_given_points(polytope\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(weights)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],points\u001b[38;5;241m=\u001b[39mX_train_blowup, give_subframes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m percent_inj_blowup \u001b[38;5;241m=\u001b[39m mcbe\u001b[38;5;241m.\u001b[39mcheck_injectivity_naive(W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(weights)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], b\u001b[38;5;241m=\u001b[39mest_alpha_blowup, points\u001b[38;5;241m=\u001b[39mX_test,\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     16\u001b[0m percent_inj_blowup_list\u001b[38;5;241m.\u001b[39mappend(percent_inj_blowup)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'mcbe' has no attribute 'be_given_points'"
     ]
    }
   ],
   "source": [
    "percent_inj_blowup_list = []\n",
    "\n",
    "for iter in range(num_evaluation_iter):\n",
    "    \n",
    "    X_train_noisy = X_train + np.random.normal(0,0.5,X_train.shape)\n",
    "    for i in range(times_to_blowup-1):\n",
    "        X_train_noisy = torch.cat((X_train_noisy,X_train + np.random.normal(0,0.5,X_train.shape)),0)\n",
    "        \n",
    "    # rowbind the original data and the noisy data by adding the noisy data to the original data\n",
    "    X_train_blowup = torch.cat((X_train,X_train_noisy),0)\n",
    "\n",
    "    est_alpha_blowup, subframes_blowup, est_points_blowup = mcbe.be_given_points(polytope=np.array(weights)[-1],points=X_train_blowup, give_subframes=True)\n",
    "\n",
    "    percent_inj_blowup = mcbe.check_injectivity_naive(W = np.array(weights)[-1], b=est_alpha_blowup, points=X_test,iter=X_test.shape[0])\n",
    "\n",
    "    percent_inj_blowup_list.append(percent_inj_blowup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate standard mcbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(num_evaluation_iter):\n",
    "    est_alpha, subframes, est_points =mcbe.mcbe(polytope=np.array(weights)[-1],N=X_train_blowup.shape[0],distribution=\"ball\",radius=np.max(np.array(X_train)), give_subframes=True,sample_on_sphere=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
